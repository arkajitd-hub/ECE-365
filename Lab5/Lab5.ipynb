{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5: Machine Translation\n",
    "\n",
    "Machine translation is an application of NLP to computationally translate text from one language to the other. It is one of the most popular fields of research in the NLP community. Very recently (2014) deep learning methods changed the face of machine translation and all of NLP. The application of deep learning to machine translation was referred to as Neural Machine Translation (NMT).\n",
    "\n",
    "In this mini lab, we will see a sample code that performs neural machine translation, and we will see the effects of various hyperparameters on the performance of the system.\n",
    "\n",
    "For simplicity, we will take as our dataset, a set of short English sentences mapped to French sentences, and instead of performing the translation one word at a time, it will be performing it one character at a time.\n",
    "\n",
    "Total points: 20 points + 10 bonus points\n",
    "\n",
    "**Submission Instructions**: Just upload this notebook, with all your answers in the respective cells, to Compass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code contains the full code of the functional NMT system. Run it to get a sense of how the loss and accuracy of the training data and the validation data are changing with every epoch. No data is necessary to download; all is included in the lab's directory.\n",
    "\n",
    "#### Packages to install:\n",
    "- pip install keras\n",
    "- pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 5.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from keras) (1.17.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: h5py in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from keras) (1.3.1)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (2.0.0a0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.14.0.dev2019030115)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.14.0a20190301)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.2 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.16.1)\n",
      "Requirement already satisfied: h5py in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow) (41.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/arkajitdutta/anaconda3/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 1.2332 - accuracy: 0.7241 - val_loss: 1.1049 - val_accuracy: 0.6965\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.9115 - accuracy: 0.7478 - val_loss: 0.9643 - val_accuracy: 0.7365\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.7925 - accuracy: 0.7855 - val_loss: 0.8345 - val_accuracy: 0.7712\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.6812 - accuracy: 0.8088 - val_loss: 0.7451 - val_accuracy: 0.7845\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.6176 - accuracy: 0.8226 - val_loss: 0.6881 - val_accuracy: 0.7983\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.5792 - accuracy: 0.8317 - val_loss: 0.6580 - val_accuracy: 0.8088\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.5524 - accuracy: 0.8388 - val_loss: 0.6347 - val_accuracy: 0.8123\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5315 - accuracy: 0.8441 - val_loss: 0.6174 - val_accuracy: 0.8175\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.5141 - accuracy: 0.8491 - val_loss: 0.5931 - val_accuracy: 0.8248\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.4988 - accuracy: 0.8537 - val_loss: 0.5844 - val_accuracy: 0.8292\n",
      "-\n",
      "Input sentence: I saw her home.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I saw her home.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I saw her swim.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I saw him jump.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I saw him once.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I saw it first.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I saw it on TV.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I saw somebody.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I saw somebody.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I saw the cook.\n",
      "Decoded sentence: Je suis pas pas de mait.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "**Summary of the algorithm**\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    It uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "**Data download**\n",
    "[English to French sentence pairs.\n",
    "](http://www.manythings.org/anki/fra-eng.zip)\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 100  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(8000,8010):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training an NMT system (or any deep learning system), one of the decisions to take is the batch size: number of examples processed (in parallel) before updating parameters.\n",
    "\n",
    "**Deliverable 1** (5 points): Change the batch size from 64 to 128, and then 256. How did it affect the speed of training (time per epoch)? Is it slower or faster? Why? What was its effect on the increase of accuracy from one epoch to the other? What is the concluded tradeoff between a smaller batch size and a larger one?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##64 \n",
    "Epoch 1/10 15s 2ms/step - loss: 1.2297 - accuracy: 0.7235 - val_loss: 1.0962 - \n",
    "val_accuracy: 0.6960\n",
    "Epoch 2/10\n",
    "8000/8000 [==============================] - 15s 2ms/step - loss: 0.9174 - accuracy: 0.7448 - val_loss: 0.9774 - val_accuracy: 0.7333\n",
    "Epoch 3/10\n",
    "8000/8000 [==============================] - 15s 2ms/step - loss: 0.8040 - accuracy: 0.7807 - val_loss: 0.8635 - val_accuracy: 0.7609\n",
    "Epoch 4/10\n",
    "8000/8000 [==============================] - 16s 2ms/step - loss: 0.6918 - accuracy: 0.8065 - val_loss: 0.7590 - val_accuracy: 0.7826\n",
    "Epoch 5/10\n",
    "8000/8000 [==============================] - 18s 2ms/step - loss: 0.6205 - accuracy: 0.8216 - val_loss: 0.6954 - val_accuracy: 0.7999\n",
    "Epoch 6/10\n",
    "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5805 - accuracy: 0.8314 - val_loss: 0.6633 - val_accuracy: 0.8058\n",
    "Epoch 7/10\n",
    "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5533 - accuracy: 0.8385 - val_loss: 0.6337 - val_accuracy: 0.8135\n",
    "Epoch 8/10\n",
    "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5319 - accuracy: 0.8439 - val_loss: 0.6158 - val_accuracy: 0.8181\n",
    "Epoch 9/10\n",
    "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5143 - accuracy: 0.8490 - val_loss: 0.5958 - val_accuracy: 0.8244\n",
    "Epoch 10/10\n",
    "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4983 - accuracy: 0.8541 - val_loss: 0.5949 - val_accuracy: 0.8233\n",
    "\n",
    "##128\n",
    "Epoch 1/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 1.4426 - accuracy: 0.7150 - val_loss: 1.1819 - val_accuracy: 0.6923\n",
    "Epoch 2/10\n",
    "8000/8000 [==============================] - 12s 2ms/step - loss: 1.0208 - accuracy: 0.7333 - val_loss: 1.0891 - val_accuracy: 0.6974\n",
    "Epoch 3/10\n",
    "8000/8000 [==============================] - 12s 2ms/step - loss: 0.9386 - accuracy: 0.7404 - val_loss: 1.0252 - val_accuracy: 0.7127\n",
    "Epoch 4/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 0.8719 - accuracy: 0.7588 - val_loss: 0.9510 - val_accuracy: 0.7400\n",
    "Epoch 5/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 0.8048 - accuracy: 0.7806 - val_loss: 0.8790 - val_accuracy: 0.7615\n",
    "Epoch 6/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7332 - accuracy: 0.7968 - val_loss: 0.8035 - val_accuracy: 0.7726\n",
    "Epoch 7/10\n",
    "8000/8000 [==============================] - 15s 2ms/step - loss: 0.6750 - accuracy: 0.8094 - val_loss: 0.7534 - val_accuracy: 0.7843\n",
    "Epoch 8/10\n",
    "8000/8000 [==============================] - 14s 2ms/step - loss: 0.6337 - accuracy: 0.8189 - val_loss: 0.7156 - val_accuracy: 0.7924\n",
    "Epoch 9/10\n",
    "8000/8000 [==============================] - 14s 2ms/step - loss: 0.6043 - accuracy: 0.8262 - val_loss: 0.6937 - val_accuracy: 0.7980\n",
    "Epoch 10/10\n",
    "8000/8000 [==============================] - 14s 2ms/step - loss: 0.5815 - accuracy: 0.8320 - val_loss: 0.6728 - val_accuracy: 0.8044\n",
    "\n",
    "##256\n",
    "Epoch 1/10\n",
    "8000/8000 [==============================] - 11s 1ms/step - loss: 1.7269 - accuracy: 0.7008 - val_loss: 1.2761 - val_accuracy: 0.6926\n",
    "Epoch 2/10\n",
    "8000/8000 [==============================] - 11s 1ms/step - loss: 1.1074 - accuracy: 0.7315 - val_loss: 1.1743 - val_accuracy: 0.6965\n",
    "Epoch 3/10\n",
    "8000/8000 [==============================] - 12s 1ms/step - loss: 1.0360 - accuracy: 0.7332 - val_loss: 1.1142 - val_accuracy: 0.6970\n",
    "Epoch 4/10\n",
    "8000/8000 [==============================] - 12s 2ms/step - loss: 0.9809 - accuracy: 0.7347 - val_loss: 1.0975 - val_accuracy: 0.6986\n",
    "Epoch 5/10\n",
    "8000/8000 [==============================] - 12s 1ms/step - loss: 0.9409 - accuracy: 0.7375 - val_loss: 1.0393 - val_accuracy: 0.7060\n",
    "Epoch 6/10\n",
    "8000/8000 [==============================] - 12s 2ms/step - loss: 0.9031 - accuracy: 0.7469 - val_loss: 1.0290 - val_accuracy: 0.7276\n",
    "Epoch 7/10\n",
    "8000/8000 [==============================] - 12s 2ms/step - loss: 0.8733 - accuracy: 0.7574 - val_loss: 0.9717 - val_accuracy: 0.7337\n",
    "Epoch 8/10\n",
    "8000/8000 [==============================] - 12s 2ms/step - loss: 0.8391 - accuracy: 0.7702 - val_loss: 0.9325 - val_accuracy: 0.7450\n",
    "Epoch 9/10\n",
    "8000/8000 [==============================] - 12s 2ms/step - loss: 0.8001 - accuracy: 0.7833 - val_loss: 0.8958 - val_accuracy: 0.7538\n",
    "Epoch 10/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7605 - accuracy: 0.7929 - val_loss: 0.8560 - val_accuracy: 0.7634\n",
    "\n",
    "\n",
    "A larger batch has a faster training time but lower accuracy. The trade-off is accuracy for a larger batch and training-time for a smaller batch.\n",
    "\n",
    "          64    vs 128    vs 256:\n",
    "Max Acc  0.8233    0.8044    0.7634\n",
    "\n",
    "Max Time  18s       15s       13s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another decision to make about the architecture of the NMT is the latent dimension size of the LSTM block. The LSTM block encodes semantics of a sentence to a vector of size equal to the latent dimension. This controls the modeling capacity of our system.\n",
    "\n",
    "**Deliverable 2** (5 points): Change the latent dimension from a 100 to a 10, and a 1000. How did it affect speed of training? Why? How did affect the accuracy of the system?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#10\n",
    "Epoch 1/10\n",
    "8000/8000 [==============================] - 5s 602us/step - loss: 4.3463 - accuracy: 0.0236 - val_loss: 4.1061 - val_accuracy: 0.0284\n",
    "Epoch 2/10\n",
    "8000/8000 [==============================] - 3s 430us/step - loss: 3.8266 - accuracy: 0.5669 - val_loss: 3.5609 - val_accuracy: 0.6891\n",
    "Epoch 3/10\n",
    "8000/8000 [==============================] - 3s 421us/step - loss: 3.2738 - accuracy: 0.7225 - val_loss: 3.0482 - val_accuracy: 0.6891\n",
    "Epoch 4/10\n",
    "8000/8000 [==============================] - 3s 421us/step - loss: 2.7633 - accuracy: 0.7225 - val_loss: 2.5910 - val_accuracy: 0.6891\n",
    "Epoch 5/10\n",
    "8000/8000 [==============================] - 4s 449us/step - loss: 2.3162 - accuracy: 0.7225 - val_loss: 2.2098 - val_accuracy: 0.6891\n",
    "Epoch 6/10\n",
    "8000/8000 [==============================] - 4s 444us/step - loss: 1.9582 - accuracy: 0.7225 - val_loss: 1.9170 - val_accuracy: 0.6891\n",
    "Epoch 7/10\n",
    "8000/8000 [==============================] - 4s 520us/step - loss: 1.6932 - accuracy: 0.7225 - val_loss: 1.7104 - val_accuracy: 0.6891\n",
    "Epoch 8/10\n",
    "8000/8000 [==============================] - 4s 455us/step - loss: 1.5048 - accuracy: 0.7225 - val_loss: 1.5528 - val_accuracy: 0.6891\n",
    "Epoch 9/10\n",
    "8000/8000 [==============================] - 4s 447us/step - loss: 1.3700 - accuracy: 0.7225 - val_loss: 1.4396 - val_accuracy: 0.6891\n",
    "Epoch 10/10\n",
    "8000/8000 [==============================] - 4s 442us/step - loss: 1.2739 - accuracy: 0.7225 - val_loss: 1.3625 - val_accuracy: 0.6891\n",
    "\n",
    "#100\n",
    "\n",
    "Epoch 1/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 1.7353 - accuracy: 0.6985 - val_loss: 1.2677 - val_accuracy: 0.6896\n",
    "Epoch 2/10\n",
    "8000/8000 [==============================] - 12s 2ms/step - loss: 1.1106 - accuracy: 0.7302 - val_loss: 1.2100 - val_accuracy: 0.6971\n",
    "Epoch 3/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 1.0463 - accuracy: 0.7333 - val_loss: 1.1265 - val_accuracy: 0.6959\n",
    "Epoch 4/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 0.9953 - accuracy: 0.7340 - val_loss: 1.0934 - val_accuracy: 0.6956\n",
    "Epoch 5/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 0.9527 - accuracy: 0.7367 - val_loss: 1.0605 - val_accuracy: 0.7128\n",
    "Epoch 6/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 0.9165 - accuracy: 0.7437 - val_loss: 1.0117 - val_accuracy: 0.7128\n",
    "Epoch 7/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 0.8846 - accuracy: 0.7556 - val_loss: 0.9782 - val_accuracy: 0.7283\n",
    "Epoch 8/10\n",
    "8000/8000 [==============================] - 14s 2ms/step - loss: 0.8465 - accuracy: 0.7685 - val_loss: 0.9580 - val_accuracy: 0.7411\n",
    "Epoch 9/10\n",
    "8000/8000 [==============================] - 14s 2ms/step - loss: 0.8123 - accuracy: 0.7803 - val_loss: 0.9147 - val_accuracy: 0.7531\n",
    "Epoch 10/10\n",
    "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7761 - accuracy: 0.7899 - val_loss: 0.8708 - val_accuracy: 0.7575\n",
    "\n",
    "#100\n",
    "Epoch 1/10\n",
    "8000/8000 [==============================] - 243s 30ms/step - loss: 1.8185 - accuracy: 0.6739 - val_loss: 1.2279 - val_accuracy: 0.6857\n",
    "Epoch 2/10\n",
    "8000/8000 [==============================] - 249s 31ms/step - loss: 1.0695 - accuracy: 0.7317 - val_loss: 1.3022 - val_accuracy: 0.6959\n",
    "Epoch 3/10\n",
    "8000/8000 [==============================] - 241s 30ms/step - loss: 1.0275 - accuracy: 0.7375 - val_loss: 1.1517 - val_accuracy: 0.7035\n",
    "Epoch 4/10\n",
    "8000/8000 [==============================] - 1830s 229ms/step - loss: 0.9707 - accuracy: 0.7508 - val_loss: 1.0648 - val_accuracy: 0.7152\n",
    "Epoch 5/10\n",
    "8000/8000 [==============================] - 316s 40ms/step - loss: 0.8715 - accuracy: 0.7729 - val_loss: 0.9341 - val_accuracy: 0.7395\n",
    "Epoch 6/10\n",
    "8000/8000 [==============================] - 332s 42ms/step - loss: 0.7906 - accuracy: 0.7889 - val_loss: 1.1096 - val_accuracy: 0.7299\n",
    "Epoch 7/10\n",
    "8000/8000 [==============================] - 277s 35ms/step - loss: 0.7047 - accuracy: 0.8021 - val_loss: 0.7530 - val_accuracy: 0.7777\n",
    "Epoch 8/10\n",
    "8000/8000 [==============================] - 297s 37ms/step - loss: 0.6726 - accuracy: 0.8125 - val_loss: 0.7172 - val_accuracy: 0.7921\n",
    "Epoch 9/10\n",
    "8000/8000 [==============================] - 349s 44ms/step - loss: 0.6167 - accuracy: 0.8220 - val_loss: 1.0190 - val_accuracy: 0.7593\n",
    "Epoch 10/10\n",
    "8000/8000 [==============================] - 317s 40ms/step - loss: 0.6362 - accuracy: 0.8240 - val_loss: 0.6927 - val_accuracy: 0.7959\n",
    "\n",
    "As the latent dimension increased the time for training exponentially but the accuracy increased with increasing number of latent dimensions. The highest accuracy of 10 vs 100 vs 1000 : 0.6891 vs 0.7575 vs 0.7959\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3** (5 points): One of the issues with the code above is that at demo time (for loop at the bottom of the code), the test is performed on training instances instead of validation instances. Adjust the code to perform the demo on the first 10 instances of the validation instances, and copy the cell's output below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Changed the range of seq_index from 100 to (8000,8010)\n",
    "\n",
    "Number of samples: 10000\n",
    "Number of unique input tokens: 70\n",
    "Number of unique output tokens: 93\n",
    "Max sequence length for inputs: 16\n",
    "Max sequence length for outputs: 59\n",
    "Train on 8000 samples, validate on 2000 samples\n",
    "Epoch 1/10\n",
    "8000/8000 [==============================] - 16s 2ms/step - loss: 1.2332 - accuracy: 0.7241 - val_loss: 1.1049 - val_accuracy: 0.6965\n",
    "Epoch 2/10\n",
    "8000/8000 [==============================] - 17s 2ms/step - loss: 0.9115 - accuracy: 0.7478 - val_loss: 0.9643 - val_accuracy: 0.7365\n",
    "Epoch 3/10\n",
    "8000/8000 [==============================] - 16s 2ms/step - loss: 0.7925 - accuracy: 0.7855 - val_loss: 0.8345 - val_accuracy: 0.7712\n",
    "Epoch 4/10\n",
    "8000/8000 [==============================] - 17s 2ms/step - loss: 0.6812 - accuracy: 0.8088 - val_loss: 0.7451 - val_accuracy: 0.7845\n",
    "Epoch 5/10\n",
    "8000/8000 [==============================] - 15s 2ms/step - loss: 0.6176 - accuracy: 0.8226 - val_loss: 0.6881 - val_accuracy: 0.7983\n",
    "Epoch 6/10\n",
    "8000/8000 [==============================] - 16s 2ms/step - loss: 0.5792 - accuracy: 0.8317 - val_loss: 0.6580 - val_accuracy: 0.8088\n",
    "Epoch 7/10\n",
    "8000/8000 [==============================] - 16s 2ms/step - loss: 0.5524 - accuracy: 0.8388 - val_loss: 0.6347 - val_accuracy: 0.8123\n",
    "Epoch 8/10\n",
    "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5315 - accuracy: 0.8441 - val_loss: 0.6174 - val_accuracy: 0.8175\n",
    "Epoch 9/10\n",
    "8000/8000 [==============================] - 20s 2ms/step - loss: 0.5141 - accuracy: 0.8491 - val_loss: 0.5931 - val_accuracy: 0.8248\n",
    "Epoch 10/10\n",
    "8000/8000 [==============================] - 18s 2ms/step - loss: 0.4988 - accuracy: 0.8537 - val_loss: 0.5844 - val_accuracy: 0.8292\n",
    "-\n",
    "Input sentence: I saw her home.\n",
    "Decoded sentence: Je suis pas pas de mait.\n",
    "\n",
    "-\n",
    "Input sentence: I saw her home.\n",
    "Decoded sentence: Je suis pas pas de mait.\n",
    "\n",
    "-\n",
    "Input sentence: I saw her swim.\n",
    "Decoded sentence: Je suis pas pas de mait.\n",
    "\n",
    "-\n",
    "Input sentence: I saw him jump.\n",
    "Decoded sentence: Je suis pas pas de mait.\n",
    "\n",
    "-\n",
    "Input sentence: I saw him once.\n",
    "Decoded sentence: Je suis pas pas de mait.\n",
    "\n",
    "-\n",
    "Input sentence: I saw it first.\n",
    "Decoded sentence: Je suis pas pas de mait.\n",
    "\n",
    "-\n",
    "Input sentence: I saw it on TV.\n",
    "Decoded sentence: Je suis pas pas de mait.\n",
    "\n",
    "-\n",
    "Input sentence: I saw somebody.\n",
    "Decoded sentence: Je suis pas pas de mait.\n",
    "\n",
    "-\n",
    "Input sentence: I saw somebody.\n",
    "Decoded sentence: Je suis pas pas de mait.\n",
    "\n",
    "-\n",
    "Input sentence: I saw the cook.\n",
    "Decoded sentence: Je suis pas pas de mait.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4** (5 points): In the following piece of code `decoder_dense = Dense(num_decoder_tokens, activation='softmax')` a dense layer is instantiated to map the output of the LSTM block to a prediction of the next character. Why is the output size of this dense layer set to `num_decoder_tokens`, which is the number of possible French output characters?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The output is the language it is being decoded in. This model is translating English into French, therefore the output needs to have the most number of possible french characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question** (10 points): During training, the model, besides being fed the target data (French translation) as in the normal supervised procedure, it is also fed the target data shifted by one position to the left. What is the purpose of this practice?  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The decoder is being fed the previous character so it can calculate the probability of the next character to be printed based on the probability of the language model. Therefore, in order to make it gramatticaly correct, the target data is being fed the word preceding it to provide more context for translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Lab5.ipynb\n",
      "a fra-eng\n",
      "a fra-eng/_about.txt\n",
      "a fra-eng/fra.txt\n",
      "a notebook.tar.gz: Can't add archive to itself\n",
      "a s2s.h5\n"
     ]
    }
   ],
   "source": [
    "!tar chvfz notebook.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
